import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_blobs
from sklearn.model_selection import ParameterGrid

# Generate sample data
X, _ = make_blobs(n_samples=1000, centers=3, cluster_std=0.5, random_state=0)

# Standardize features use
X = StandardScaler().fit_transform(X)

# Define the parameter grid
param_grid = {
    'eps': np.arange(0.1, 1.5, 0.1),
    'min_samples': range(2, 20)
}

# Create a function to perform grid search
def grid_search_dbscan(X, param_grid):
    best_score = -1
    best_params = None
    best_model = None
    
    for params in ParameterGrid(param_grid):
        model = DBSCAN(eps=params['eps'], min_samples=params['min_samples'])
        labels = model.fit_predict(X)
        
        # Silhouette score is not defined for single cluster
        if len(set(labels)) > 1:
            score = silhouette_score(X, labels)
            if score > best_score:
                best_score = score
                best_params = params
                best_model = model
    
    return best_model, best_params, best_score

# Perform the grid search
best_model, best_params, best_score = grid_search_dbscan(X, param_grid)

print(f"Best parameters: {best_params}")
print(f"Best silhouette score: {best_score}")

# Plot the best clustering result
import matplotlib.pyplot as plt

labels = best_model.labels_
unique_labels = set(labels)

colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]
for k, col in zip(unique_labels, colors):
    if k == -1:
        col = [0, 0, 0, 1]  # Black used for noise.

    class_member_mask = (labels == k)

    xy = X[class_member_mask & core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
             markeredgecolor='k', markersize=14)

    xy = X[class_member_mask & ~core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
             markeredgecolor='k', markersize=6)

plt.title('Estimated number of clusters: %d' % len(unique_labels))
plt.show()






import numpy as np
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import DBSCAN
from sklearn.decomposition import PCA
from sklearn.datasets import fetch_20newsgroups

# Load some text data
newsgroups = fetch_20newsgroups(subset='all', categories=['sci.space', 'comp.graphics'])
texts = newsgroups.data

# Convert text data to numerical features using TfidfVectorizer
vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(texts)

# Apply DBSCAN clustering
dbscan = DBSCAN(eps=0.5, min_samples=5, metric='cosine')
labels = dbscan.fit_predict(X)

# Use PCA to reduce the dimensionality for visualization
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X.toarray())

# Plot the results
plt.figure(figsize=(10, 6))
unique_labels = set(labels)
colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]

for k, col in zip(unique_labels, colors):
    if k == -1:
        col = [0, 0, 0, 1]  # Black used for noise.
    
    class_member_mask = (labels == k)
    xy = X_pca[class_member_mask]
    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
             markeredgecolor='k', markersize=6)

plt.title('DBSCAN clustering on text data (using TfidfVectorizer)')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.show()
